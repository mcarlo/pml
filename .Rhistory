library(rattle)
prettyRplot(tree2)
fancyRpartPlot(tree2)
fancyRpartPlot(tree1)
print tree1
print(tree1)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
train <- subset(segmentationOriginal, Case = "Train")
test <- subset(segmentationOriginal, Case = "Test")
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
#1. Subset the data to a training set and testing set based on the Case variable in the data set.
train <- subset(segmentationOriginal, Case = "Train")
test <- subset(segmentationOriginal, Case = "Test")
set.seed(125)
modFit <- train(Class ~ .,method="rpart",data=train)
plot(modFit$finalModel, uniform=TRUE,
main="Classification Tree")
print(modFit$finalModel)
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
library(rattle)
fancyRpartPlot(modFit$finalModel)
print(modFit)
plot(modFit$finalModel, uniform=TRUE,
main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
print(modFit$finalModel)
testA <- test[1,]
testA$TotalIntench2 = 23000; testA$FiberWidthCh1 = 10; testA$PerimStatusCh1=2;
predict(modFit$finalModel, newdata = testA)
predict(modFit$finalModel, newdata = testA, type = "class")
str(testA)
colnames(train)[1:10]
train <- train [, -2]
test <- test [, -2]
set.seed(125)
modFit <- train(Class ~ .,method="rpart",data=train)
plot(modFit$finalModel, uniform=TRUE,
main="Classification Tree")
print(modFit$finalModel)
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
library(rattle)
fancyRpartPlot(modFit$finalModel)
testA <- test[1,]
testA$TotalIntench2 = 23000; testA$FiberWidthCh1 = 10; testA$PerimStatusCh1=2;
predict(modFit$finalModel, newdata = testA, type = "class")
testB$TotalIntench2 = 50000; testB$FiberWidthCh1 = 10; testB$VarIntenCh4 = 100
testB <- test[2,]
testB$TotalIntench2 = 50000; testB$FiberWidthCh1 = 10; testB$VarIntenCh4 = 100
predict(modFit$finalModel, newdata = testB, type = "class")
testC <- test[3,]
testC$TotalIntench2 = 57,000; testC$FiberWidthCh1 = 8;testC$VarIntenCh4 = 100;
testC$TotalIntench2 = 57000; testC$FiberWidthCh1 = 8;testC$VarIntenCh4 = 100;
predict(modFit$finalModel, newdata = testC, type = "class")
testD <- test[4,]
testD$TotalIntench2 = NA; testD$FiberWidthCh1 = 8;testD$VarIntenCh4 = 100; testD$PerimStatusCh1=2 ;
predict(modFit$finalModel, newdata = testD, type = "class")
treeCell <- tree(Class ~ ., data = train)
plot(treeCell, uniform=TRUE,
main="Classification Tree")
?tree
set.seed(125)
modFit <- train(Class ~ .,data=train)
set.seed(125)
modFit <- train(Class ~ .,data=train, method = "rpart")
print(modFit$finalModel)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
train <- subset(segmentationOriginal, Case = "Train")
train <- train [, -2]
test <- subset(segmentationOriginal, Case = "Test")
test <- test [, -2]
set.seed(125)
modFit <- train(Class ~ .,data=train, method = "rpart")
plot(modFit)
plot(modFit$finalModel)
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=1.8)
plot(modFit$finalModel)
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=1.)
library(rattle)
fancyRpartPlot(modFit$finalModel)
set.seed(125)
modFit <- fit(Class ~ .,data=train, method = "rpart")
set.seed(125)
modFit <- fit(Class ~ .,data=train, method = "rpart2")
set.seed(125)
modFit <- train(Class ~ .,data=train, method = "rpart2")
print(modFit$finalModel)
plot(modFit$finalModel)
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=1.)
fancyRpartPlot(modFit$finalModel)
testA <- test[1,]
testA$TotalIntench2 = 23000; testA$FiberWidthCh1 = 10; testA$PerimStatusCh1=2;
predict(modFit$finalModel, newdata = testA, type = "class")
testB <- test[2,]
testB$TotalIntench2 = 50000; testB$FiberWidthCh1 = 10; testB$VarIntenCh4 = 100
predict(modFit$finalModel, newdata = testB, type = "class")
testC <- test[3,]
testC$TotalIntench2 = 57000; testC$FiberWidthCh1 = 8;testC$VarIntenCh4 = 100;
predict(modFit$finalModel, newdata = testC, type = "class")
testD <- test[4,]
testD$TotalIntench2 = NA; testD$FiberWidthCh1 = 8;testD$VarIntenCh4 = 100; testD$PerimStatusCh1=2 ;
predict(modFit$finalModel, newdata = testD, type = "class")
plot(modFit$finalModel)
print(modFit$finalModel)
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=1.)
fancyRpartPlot(modFit$finalModel)
set.seed(125)
modFit <- train(Class ~ .,data=train, method = "rpart")
train <- subset(segmentationOriginal, Case = "Train")
set.seed(125)
modFit <- train(Class ~ .,data=train, method = "rpart")
set.seed(125)
modFit <- train(Class ~ .,data=segmentationOriginal, subset = Case=="Train", method = "rpart")
print(modFit$finalModel)
fancyRpartPlot(modFit$finalModel)
testA <- test[1,]
testA$TotalIntench2 = 23000; testA$FiberWidthCh1 = 10; testA$PerimStatusCh1=2;
predict(modFit$finalModel, newdata = testA, type = "class")
library("forecast", lib.loc="~/R/win-library/3.1")
library(ElemStatLearn);library(caret);library(gbm)
training <- vowel.train
testing <- vowel.test
training$y <- as.factor(training$y)
testing$y <- as.factor(testing$y)
set.seed(33833)
mod1 <- train(y ~.,method="rf",data=training)
set.seed(33833)
mod2 <- train(y ~.,method="gbm",data=training)
pred1 <- predict(mod1,testing); pred2 <- predict(mod2,testing)
confusionMatrix(pred1, testing$y)$overall
confusionMatrix(pred2, testing$y)$overall
confusionMatrix(pred1, pred2)$overall
confusionMatrix(pred1, testing$y)$overall[1]
confusionMatrix(pred2, testing$y)$overall[1]
confusionMatrix(pred1, pred2)$overall[1]
mod2 <- train(y ~.,method="gbm",data=training)
pred1 <- predict(mod1,testing); pred2 <- predict(mod2,testing)
confusionMatrix(pred1, testing$y)$overall[1]
confusionMatrix(pred2, testing$y)$overall[1]
confusionMatrix(pred1, pred2)$overall[1]
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
mod1 <- train(diagnosis ~.,method="rf",data=training)
mod2 <- train(diagnosis ~.,method="gbm",data=training)
mod3 <- train(diagnosis ~.,method="lda",data=training)
pred1 <- predict(mod1,testing); pred2 <- predict(mod2,testing);
pred3 <- predict(mod3,testing)
confusionMatrix(pred1, testing$diagnosis)$overall[1]
confusionMatrix(pred2, testing$diagnosis)$overall[1]
confusionMatrix(pred3, testing$diagnosis)$overall[1]
predDF <- data.frame(pred1,pred2,pred3,diagnosis=testing$diagnosis)
combModFit <- train(diagnosis ~.,method="rf",data=predDF)
sum(predDF$pred2 -predDF$pred3)
sum(1*(predDF$pred2 !=predDF$pred3))
sum(1*(predDF$pred2 !=predDF$pred1))
sum(1*(predDF$pred3 !=predDF$pred1))
combModFit
confusionMatrix(combPred, testing$diagnosis)$overall[1]
combPred <- predict(combModFit,predDF)
confusionMatrix(combPred, testing$diagnosis)$overall[1]
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(233)
library(lars)
covnames <- names(training[,-9])
lasso.fit <- lars(as.matrix(training[,-9]), training$CompressiveStrength, type="lasso", trace=TRUE)
plot(lasso.fit, breaks=FALSE)
legend("topleft", covnames, pch=8, lty=1:length(covnames), col=1:length(covnames))
dev.off()
plot(lasso.fit, breaks=FALSE)
legend("topleft", covnames, pch=8, lty=1:length(covnames), col=1:length(covnames))
plot(lasso.fit, breaks=FALSE)
legend("topright", covnames, pch=8, lty=1:length(covnames), col=1:length(covnames))
lasso.cv <- cv.lars(as.matrix(training[,-9]), training$CompressiveStrength, K=10, type="lasso", trace=TRUE)
plot(lasso.fit, breaks=FALSE)
legend("topright", covnames, pch=8, lty=1:length(covnames), col=1:length(covnames))
plot(lasso.fit, breaks=FALSE)
legend("topright", covnames, pch=8, lty=1:length(covnames), col=1:length(covnames))
library(lubridate)  # For year() function below
dat = read.csv("gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
tstest <- ts(testing$visitsTumblr)
library(lubridate)  # For year() function below
dat = read.csv("gaData.csv")
setwd("~/GitHub/pml")
dat = read.csv("gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
tstest <- ts(testing$visitsTumblr)
library(forecast)
bats1 <- bats(tstrain)
predBats1 <- forecast.bats(bats1, h = 235, level = .95)
accuracy(predBats1, tstest)
predBats1 <- forecast(bats1)
accuracy(predBats1, tstest)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(325)
svm1 <- svm(CompressiveStrength ~ ., data = training)
predSVM <- predict.svm(svm1, newdata = testing)
predSVM <- predict(svm1, newdata = testing)
?resid
resid1 <- predSVM - testing$CompressiveStrength
sqrt(sum(resid1*resid1))
sum(resid1*resid1)/256
library(lubridate)  # For year() function below
dat = read.csv("gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
tstest <- ts(testing$visitsTumblr)
library(forecast)
bats1 <- bats(tstrain)
predBats1 <- forecast(bats1, h=c(235))
accuracy(predBats1, tstest)
predBats1
predBats1 <- forecast(bats1, h=c(235), level = 0.95)
predBats1
predBats1 <- forecast(bats1, h=c(235))
predBats1[,5]
predBats1
accuracy(predBats1, tstest)
accuracy(predBats1$mean, tstest)
plot(predBats1$lower[,2])
plot(predBats1$upper[,2])
1*(tstest < predBats1$upper[,2])
mean(1*(tstest <= predBats1$upper[,2])*(tstest >= predBats1$lower[,2]))
library(ElemStatLearn);library(caret);library(gbm)
training <- vowel.train
testing <- vowel.test
training$y <- as.factor(training$y)
testing$y <- as.factor(testing$y)
set.seed(33833)
mod1 <- train(y ~.,method="rf",data=training)
mod2 <- train(y ~.,method="gbm",data=training)
pred1 <- predict(mod1,testing); pred2 <- predict(mod2,testing)
agree <- pred1[pred1 == pred2]
testAgree <- testing[pred1 == pred2]
testAgree <- testing[pred1 == pred2,]
confusionMatrix(agree, testAgree$y)$overall[1]
?rmse
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(325)
svm1 <- svm(CompressiveStrength ~ ., data = training)
predSVM <- predict(svm1, newdata = testing)
confusionMatrix(predSVM, testing$CompressiveStrength)
residSS <- sum((predSVM - testing$CompressiveStrength)^2)/255
residSS <- sqrt(sum((predSVM - testing$CompressiveStrength)^2)/255)
residSS <- sqrt(sum((predSVM - testing$CompressiveStrength)^2)) #/255)
predSVM2 <- predict(svm1, testing)
residSS <- sqrt(sum((predSVM - testing$CompressiveStrength)^2)/255)
residSS <- sum((predSVM - testing$CompressiveStrength)^2)/255
residSS <- sqrt(sum((predSVM - testing$CompressiveStrength)^2)/255)
residSS <- sum((predSVM - testing$CompressiveStrength)^2)/255
residSS2 <- sqrt(sum((predSVM-testing$CompressiveStrength)^2)/length(predSVM))
setwd("~/GitHub/pml")
training <- read.csv("~/GitHub/pml/pml-training.csv")
testing <- read.csv("~/GitHub/pml/pml-testing.csv")
colnames(training)
library(knitr); library(caret); library(party);library(ElemStatLearn);
library(MASS);library(plyr); library(gbm); library(survival); library(splines);
library(randomForest)
allColumns <- colnames(training)
availColumns <- cbind(colnames(training),apply(testing, 2, max))
availColumns <- availColumns[is.na(availColumns[,2])==FALSE ,1]
availTrain <- training[, cbind(availColumns)[-c(3, 4, 5, 6)]]
availTest <- testing[, colnames(testing) %in% colnames(availTrain)]
trainRows <- nrow(availTrain)
predictions <- 100
testRows <- nrow(availTest)
rfC <- train(as.factor(classe) ~ . , data = availTrain, method = "rf",
importance = T)
predC <- predict(rfC, newdata = availTest)
predC
rfC$finalModel
str(rfC$finalModel)
availA <- availTrain[availTrain$user_name == "adelmo",3:56]
trainA <- train(as.factor(classe) ~ . , data = availA, method = "rf",
importance = T,prox=TRUE)
trainA <- train(as.factor(classe) ~ . , data = availA, method = "rpart",
importance = T,prox=TRUE)
warnings()
trainRows <- nrow(availTrain)
predictions <- 100
testRows <- nrow(availTest)
predictionMatrix <- matrix(rep(0, 100), nrow = 20, ncol = 5)
rm(rfApred)
predictions <- 10
testRows <- nrow(availTest)
for (i in 1:predictions){
endCol <- 5*i
beginCol <- endCol - 4
set.seed(i)
sample.split <- sample(1:trainRows,size=500,replace=F)
trainSample <- availTrain[sample.split,]
rfA <- randomForest(as.factor(classe) ~ . , data = trainSample, ntree = 100,type="class")
rfApred[, beginCol:endCol] <- predict(rfA,newdata=availTest,type="class")
}
rfApred <- matrix(rep("E", predictions * 100), nrow = 20, ncol = 5*predictions)
for (i in 1:predictions){
endCol <- 5*i
beginCol <- endCol - 4
set.seed(i)
sample.split <- sample(1:trainRows,size=500,replace=F)
trainSample <- availTrain[sample.split,]
rfA <- randomForest(as.factor(classe) ~ . , data = trainSample, ntree = 100,type="class")
rfApred[, beginCol:endCol] <- predict(rfA,newdata=availTest,type="class")
}
rfApred
rfApred <- matrix(rep(5, predictions * 100), nrow = 20, ncol = 5*predictions)
for (i in 1:predictions){
endCol <- 5*i
beginCol <- endCol - 4
set.seed(i)
sample.split <- sample(1:trainRows,size=500,replace=F)
trainSample <- availTrain[sample.split,]
rfA <- randomForest(as.factor(classe) ~ . , data = trainSample, ntree = 100,type="class")
rfApred[, beginCol:endCol] <- predict(rfA,newdata=availTest,type="class")
}
rfApred
for (i in 1:predictions){
endCol <- 5*i
beginCol <- endCol - 4
set.seed(i)
sample.split <- sample(1:trainRows,size=1000,replace=F)
trainSample <- availTrain[sample.split,]
rfA <- randomForest(as.factor(classe) ~ . , data = trainSample, ntree = 100,type="class")
rfApred[, beginCol:endCol] <- predict(rfA,newdata=availTest,type="class")
}
rfApred
for (i in 1:predictions){
endCol <- 5*i
beginCol <- endCol - 4
set.seed(i)
sample.split <- sample(1:trainRows,size=1000,replace=F)
trainSample <- availTrain[sample.split,]
rfA <- randomForest(as.factor(classe) ~ . , data = trainSample, ntree = 500,type="class")
rfApred[, beginCol:endCol] <- predict(rfA,newdata=availTest,type="class")
}
rfApred
predict(rfA,newdata=availTest,type="class")
predictionMatrix <- matrix(rep(0.2, 100), nrow = 20, ncol = 5)
rfApred <- matrix(rep(0.2, predictions * 100), nrow = 20, ncol = 5*predictions)
for (i in 1:predictions){
endCol <- 5*i
beginCol <- endCol - 4
set.seed(i)
sample.split <- sample(1:trainRows,size=1000,replace=F)
trainSample <- availTrain[sample.split,]
rfA <- randomForest(as.factor(classe) ~ . , data = trainSample, ntree = 500,type="prob")
rfApred[, beginCol:endCol] <- predict(rfA,newdata=availTest,type="prob")
}
predict(rfA,newdata=availTest,type="class")
predict(rfA,newdata=availTest,type="prob")
rfApred
modFit <- train(classe ~ ., method="gbm",data=availA,verbose=FALSE)
testA <- availTest[availTest$user_name == "adelmo", ]
predA <- predict(modFit, newdata = testA)
predA <- predict(modFit, newdata = availTest)
table(predA)
predA <- predict(modFit, newdata = availTrain)
table(predA)
table(predA, availTrain$classe)
print(modFit)
?gbm
setwd("~/GitHub/pml")
training <- read.csv("~/GitHub/pml/pml-training.csv")
testing <- read.csv("~/GitHub/pml/pml-testing.csv")
colnames(training)
library(knitr); library(caret); library(party);library(ElemStatLearn);
library(MASS);library(plyr); library(gbm); library(survival); library(splines);
library(randomForest)
allColumns <- colnames(training)
# id columns that are not NA in testing data set
availColumns <- cbind(colnames(training),apply(testing, 2, max))
availColumns <- availColumns[is.na(availColumns[,2])==FALSE ,1]
availTrain <- training[, cbind(availColumns)[-c(3, 4, 5, 6)]]
availTest <- testing[, colnames(testing) %in% colnames(availTrain)]
availTrain <- training[, cbind(availColumns)[-c(1,3:6)]]
availTest <- testing[, colnames(testing) %in% colnames(availTrain)]
trainRows <- nrow(availTrain)
predictions <- 10
testRows <- nrow(availTest)
modGBM <- train(classe ~ ., method="gbm",data=availTrain,verbose=FALSE,
type = "prob")
modRF <- train(classe ~ ., method="gbm",data=availTrain,verbose=FALSE,
type = "prob")
warnings()
modGBM <- train(classe ~ ., method="gbm",data=availTrain,verbose=FALSE)
modRF <- train(classe ~ ., method="rf",data=availTrain,verbose=FALSE,
type = "prob")
predict(modGBM, newdata = availTest)
predict(modRF, newdata = availTest)
confusionMatrix(modGB, modRF)
confusionMatrix(predict(modGB, newdata = availTest), predict(modRF, newdata = availTest))
confusionMatrix(predict(modGBM, newdata = availTest), predict(modRF, newdata = availTest))
answers <- predict(modGBM, newdata = availTest)
getwd()
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
pml_write_files(answers)
ls()
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
if (i < 10){
filename = paste0("problem_id_",0,i,".txt")
}
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
if (i < 10){
filename = paste0(paste("problem_id_",0,sep=""),i,".txt")
}
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
?paste0
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
if (i < 10){
prefix = paste("problem_id_","0",sep="")
filename = paste0(prefix,i,".txt")
}
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
getwd()
ls()
rfModel <- load("modRF.RData")
modGBM
modRF
sampleClasse <- createDataPartition(y=availTrain$classe,
p=0.2, list=FALSE)
library(knitr); library(caret); library(randomForest); library(gbm);
sampleClasse <- createDataPartition(y=availTrain$classe,
p=0.2, list=FALSE)
sampleUser <- createDataPartition(y=sampleClasse$user_name,
p=0.25, list=FALSE)
sampleC <- availTrain[sampleClasse,]
sampleUser <- createDataPartition(y=sampleC$user_name,
p=0.25, list=FALSE)
sampleU <- sampleC[sampleUser,]
sampleRF <- train(classe ~ ., method="gbm",data=sampleU,verbose=FALSE)
sampleRF <- train(classe ~ ., method="gbm",data=sampleU[,c(1:10,55)],verbose=FALSE)
sampleRF
sampleRF <- train(classe ~ ., method="rf",data=sampleU[,c(1:10,55)],verbose=FALSE)
sampleRF
sampleRFa <- train(classe ~ ., method="rf",data=sampleU[,c(11:27,55)],verbose=FALSE)
sampleRFa
sampleRFb <- train(classe ~ ., method="rf",data=sampleU[,c(28:41,55)],verbose=FALSE)
sampleRFb
sampleRFc <- train(classe ~ ., method="rf",data=sampleU[,c(42:54,55)],verbose=FALSE)
sampleRFc
availRF <- train(classe ~ ., method="rf",data=availTrain[,c(1:10,55)],verbose=FALSE)
availRF <- train(classe ~ ., method="rf",data=availTrain[sampleClasse,c(1:10,55)],verbose=FALSE)
availRF
sys.time()
predict(availRf, newdata = availTrain)
availRF
predict(availRF)
predict(availRf, newdata = availTrain[,c(1:10,55)])
predict(availRF, newdata = availTrain[,c(1:10,55)])
confusionMatrix(predict(availRF, newdata = availTrain[,c(1:10,55), availTrain$classe])
)
confusionMatrix(predict(availRF, newdata = availTrain[,c(1:10,55), availTest$classe])
)
predict(availRF, newdata = availTest
)
varImp(availRF)
