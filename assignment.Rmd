---
title: "Predicting Exercise Class from Activity Measurements"
author: "TMA"
date: "Thursday, August 14, 2014"
output:
  html_document:
    highlight: kate
    number_sections: yes
    theme: cerulean
---
========================================

# Synopsis
The goal of this task was to predict the manner of exercise used by participants in the Weight Lifting Exercises (WLE) Dataset <http://groupware.les.inf.puc-rio.br/har>.

The WLE data describe an experiment in which the experimenters measured activity on six human participants performing the exercise called "dumbbell biceps curls." The experimenters instructed the participants to perform the exercises either properly or improperly, as indicated by five different classes. Class A indicated they performed properly, Classes B, C, D, and E indicated they performed the exercise improperly in specific ways.

The experimenters measured participants' activity from sensors in four locations, including three wearable devices. The measurement locations were: on the dumbbells, on a belt worn around the waist, on an arm-band, and on a glove. Each device's metrics included three axis measures from an accelerometer, a gyroscope, and a magnetometer.

To read more, visit the WLE website: <http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf>.

# Data Processing
The data consists of a preprocessed training data set including 19,622 observations among 160 variables, and a testing data set including 20 observations. The testing data did not label the "classe" variable, the discovery of which was the purpose of this analysis. 


```{r rsubset,cache=TRUE,echo=FALSE}
library(knitr); library(caret); library(randomForest); library(gbm);
library(party);library(ElemStatLearn); library(xtable);
library(MASS);library(plyr); library(survival); library(splines);

setwd("~/GitHub/pml")
training <- read.csv("~/GitHub/pml/pml-training.csv")
testing <- read.csv("~/GitHub/pml/pml-testing.csv")

allColumns <- colnames(training)

# id columns that are not NA in testing data set
availColumns <- cbind(colnames(training),apply(testing, 2, max))
availColumns <- availColumns[is.na(availColumns[,2])==FALSE ,1]
availTrain <- training[, cbind(availColumns)[-c(1,3:6)]]
availTest <- testing[, colnames(testing) %in% colnames(availTrain)]

trainRows <- nrow(availTrain)

wleVariables <- matrix(rep("", 52), nrow=13, ncol = 4)
wleVariables[,1] <- colnames(availTrain)[3:15]
wleVariables[,2] <- colnames(availTrain)[16:28]
wleVariables[,3] <- colnames(availTrain)[29:41]
wleVariables[,4] <- colnames(availTrain)[42:54]
colnames(wleVariables) <- c("Belt", "Arm", "Dumbbells", "Forearm")

set.seed(80302)
subRows1 <- sample(1:trainRows, trainRows/5)
subSet1 <- availTrain[subRows1,]

subRows2 <- sample((1:trainRows)[!(row(availTrain)[,1] %in% subRows1)], (trainRows - length(subRows1))/4)
subSet2 <- availTrain[subRows2,]

subRows3 <- sample((1:trainRows)[!(row(availTrain)[,1] %in% c(subRows1, subRows2))], (trainRows - length(subRows1) - length(subRows2))/3)
subSet3 <- availTrain[subRows3,]

subRows4 <- sample((1:trainRows)[!(row(availTrain)[,1] %in% c(subRows1, subRows2,subRows3))], (trainRows - length(subRows1) - length(subRows2) - length(subRows3))/2)
subSet4 <- availTrain[subRows4,]

subSet5 <- availTrain[(1:trainRows)[!(row(availTrain)[,1] %in% c(subRows1, subRows2,subRows3,subRows4))],]

```

I intended to model the data using random forests of various sizes. To eliminate variables not useful in the testing data, I subset the training data to include only variables that did not have "NA" in the testing data, leaving 52 possible predictor variables. These possible predictor variables comprised 13 variables for each of the four measurement locations.

```{r rfSub2,cache=FALSE,echo=FALSE,dependson='rsubset',results='asis'}
library(xtable)
xtab1 <- xtable(wleVariables)
print(xtab1, type = "html", include.rownames = FALSE)
```

Given an intent to use randomForest as the preferred method of prediction, I split the training data equally among five subsets by randomly sampling the "classe" (outcome) variable. 

```{r rfMod,cache=TRUE,echo=FALSE,dependson='rsubset'}
set.seed(60614)
rfBelt <- train(classe ~ ., method="rf",data=subSet1[,c(3:15,55)],verbose=FALSE)
rfArm <- train(classe ~ ., method="rf",data=subSet1[,c(16:28,55)],verbose=FALSE)
rfDumbell <- train(classe ~ ., method="rf",data=subSet1[,c(29:41,55)],verbose=FALSE)
rfForearm <- train(classe ~ ., method="rf",data=subSet1[,c(42:54,55)],verbose=FALSE)
rfTrain  <- train(classe ~ ., method="rf",data=availTrain[,c(3:54,55)],verbose=FALSE)
```

# Modeling: mini-random forests to identify locations' most imporant variables
Random forest is a powerful method for predicting classification. One of its drawbacks is computation time. 

I wanted to predict the error rate on future test sets by training a random forest model on the entire training set. The random forest method grows trees randomly on subsets of the training data and selects the best one based on out of the bag (OOB) cross-validation error rates, so there is no need to conduct an additional cross validation exercise outside the random forest process.

However, I was curious how close I could come to the error rate from the model built using the entire training data by combining the best variables identified from subsets of the variables. I trained four random forests on the first training subset, with each trained model using metrics from only one of the four locations. These random forest models were named _rfBelt_, _rfArm_, _rfDumbell_, and _rfForearm_, where _rfForearm_ referred to measurements at the "glove" location and _rfDumbell_ was subconcsiously mispelled.

Among the four mini random forest models, the following variables met the threshold of variable importance greater than 50.

```{r rfInspect,cache=TRUE,echo=FALSE,dependson=c('rsubset','rfMod'),results='asis'}
library(xtable)

varImpOverall <- rbind(varImp(rfBelt)$importance, varImp(rfArm)$importance, varImp(rfDumbell)$importance, varImp(rfForearm)$importance)

importantVariables2a <- rownames(varImpOverall)[varImpOverall$Overall > 50]

importantVariables2 <- c(rownames(varImpOverall)[varImpOverall$Overall > 50],"classe")

impVar2Matrix <- matrix(importantVariables2a, ncol = 1)
xtable2 <- xtable(impVar2Matrix)

print(xtable2, type = 'html', include.rownames = FALSE)

```

```{r rftest,cache=TRUE,echo=FALSE,dependson=c('rsubset','rfMod','rfInspect')}

system.time(rfImportant <- train(classe ~ ., method="rf",data=subSet1[,importantVariables2],verbose=FALSE))
```

```{r rfTest2,cache=TRUE,echo=TRUE,dependson=c('rsubset','rfMod','rftest')}
rfTrain2  <- train(classe ~ ., method="rf",data=availTrain[,importantVariables2],verbose=FALSE)

```

Running  OOB error rate for the selected model was 0.025. That is my expectation for the error rate on any test sets.

To verify this expectation, I tested the final model on training subsets 2, 3, 4, and 5. Their error rates ranged from to .

```{r sumRF2,cache=TRUE,echo=TRUE,dependson=c('rsubset','rfMod','rftest', 'rfTest2')}
library(caret)

rfImportant
rfTrain
rfTrain2
varImp(rfTrain)
varImp(rfTrain2)

```

I would predict accuracy of 0.98 on the testing data. The chance of predicting all 20 correctly is 0.98^20, or 0.67.