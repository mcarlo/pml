---
title: "Predicting Exercise Class from Activity Measurements"
author: "TMA"
date: "Thursday, August 14, 2014"
output:
  html_document:
    highlight: kate
    number_sections: yes
    theme: cerulean
---
========================================

# Synopsis
The goal of this task was to predict the manner of exercise used by participants in the Weight Lifting Exercises (WLE) Dataset <http://groupware.les.inf.puc-rio.br/har>.

The WLE data describe different an experiment in which the experimenters measured activity on six human participants performing the exercise called "dumbbell biceps curls." The experimenters instructed the participants to perform the exercises either properly or improperly, as indicated by five different classes. Class A indicated they performed properly, Classes B, C, D, and E indicated they performed the exercise improperly in specific ways.

The experimenters measured participants' activity from sensors in four locations, including three wearable devices. The measurement locations were: on the dumbbells, on a belt, on an arm-band, and on a glove. Each device's metrics included three axis measures from an accelerometer, a gyroscope, and a magnetometer.

To read more, visit the WLE website: <http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf>.

# Data Processing
The data consists of a preprocessed training data set including 19,622 observations among 160 variables, and a testing data set including 20 observations. The testing data did not label the "classe" variable, the discovery of which was the purpose of this analysis. 


```{r rsubset,cache=TRUE,echo=FALSE}
library(knitr); library(caret); library(randomForest); library(gbm);
library(party);library(ElemStatLearn); library(xtable);
library(MASS);library(plyr); library(survival); library(splines);

setwd("~/GitHub/pml")
training <- read.csv("~/GitHub/pml/pml-training.csv")
testing <- read.csv("~/GitHub/pml/pml-testing.csv")

allColumns <- colnames(training)

# id columns that are not NA in testing data set
availColumns <- cbind(colnames(training),apply(testing, 2, max))
availColumns <- availColumns[is.na(availColumns[,2])==FALSE ,1]
availTrain <- training[, cbind(availColumns)[-c(1,3:6)]]
availTest <- testing[, colnames(testing) %in% colnames(availTrain)]

trainRows <- nrow(availTrain)

wleVariables <- matrix(rep("", 52), nrow=13, ncol = 4)
wleVariables[,1] <- colnames(availTrain)[3:15]
wleVariables[,2] <- colnames(availTrain)[16:28]
wleVariables[,3] <- colnames(availTrain)[29:41]
wleVariables[,4] <- colnames(availTrain)[42:54]
colnames(wleVariables) <- c("Belt", "Arm", "Dumbbells", "Forearm")

set.seed(80302)
subRows1 <- sample(1:trainRows, trainRows/5)
subSet1 <- availTrain[subRows1,]

subRows2 <- sample((1:trainRows)[!(row(availTrain)[,1] %in% subRows1)], (trainRows - length(subRows1))/4)
subSet2 <- availTrain[subRows2,]

subRows3 <- sample((1:trainRows)[!(row(availTrain)[,1] %in% c(subRows1, subRows2))], (trainRows - length(subRows1) - length(subRows2))/3)
subSet3 <- availTrain[subRows3,]

subRows4 <- sample((1:trainRows)[!(row(availTrain)[,1] %in% c(subRows1, subRows2,subRows3))], (trainRows - length(subRows1) - length(subRows2) - length(subRows3))/2)
subSet4 <- availTrain[subRows4,]

subSet5 <- availTrain[(1:trainRows)[!(row(availTrain)[,1] %in% c(subRows1, subRows2,subRows3,subRows4))],]

```

To eliminate variables not useful in the testing data, I subset the training data to include only variables that did not have "NA" in the testing data, leaving 52 possible predictor variables. These possible predictor variables comprised 13 variables for each of the four measurement locations.

```{r rfSub2,cache=FALSE,echo=FALSE,dependson='rsubset',results='asis'}
library(xtable)
xtab1 <- xtable(wleVariables)
print(xtab1, type = "html", include.rownames = FALSE)
```

Given an intent to use randomForest as the preferred method of prediction, I split the training data equally among five subsets by randomly sampling the "classe" (outcome) variable. 

```{r rfMod,cache=TRUE,echo=FALSE,dependson='rsubset'}
set.seed(60614)
rfBelt <- train(classe ~ ., method="rf",data=subSet1[,c(3:15,55)],verbose=FALSE)
rfArm <- train(classe ~ ., method="rf",data=subSet1[,c(16:28,55)],verbose=FALSE)
rfDumbell <- train(classe ~ ., method="rf",data=subSet1[,c(29:41,55)],verbose=FALSE)
rfForearm <- train(classe ~ ., method="rf",data=subSet1[,c(42:54,55)],verbose=FALSE)
rfTrain  <- train(classe ~ ., method="rf",data=availTrain[,c(3:54,55)],verbose=FALSE)
```

# Modeling: mini-random forests to identify locations' most imporant variables
Random forest is a powerful method for predicting classification. One of its drawbacks is computation time. One unverified estimate of its complexity is 
_O(n_ * _log(n))_. (See <http://stats.stackexchange.com/questions/37370/random-forest-computing-time-in-r>.)

To make this analysis more manageable, I trained four random forests on the first training subset, with each trained model using metrics from only one of the four locations. These random forest models were named _rfBelt_, _rfArm_, _rfDumbell_, and _rfForearm_, where _rfForearm_ referred to measurements at the "glove" location and _rfDumbell_ was subconcsiously mispelled.

```{r rfInspect,cache=TRUE,echo=FALSE,dependson=c('rsubset','rfMod')}
varImpOverall <- rbind(varImp(rfBelt)$importance, varImp(rfArm)$importance, varImp(rfDumbell)$importance, varImp(rfForearm)$importance)
```

```{r rftest,cache=TRUE,echo=FALSE,dependson=c('rsubset','rfMod','rfInspect')}
importantVariables2 <- c(rownames(varImpOverall)[varImpOverall$Overall > 50],"classe")
#xtable2 <- xtable(matrix(importantVariables2[-15], ncol = 1))

#print(xtable2, type = 'html', include.rownames = FALSE)

system.time(rfImportant <- train(classe ~ ., method="rf",data=subSet1[,importantVariables2],verbose=FALSE))
```

The following variables met the threshold of variable importance greater than 50.


```{r rfTest2,cache=TRUE,echo=TRUE,dependson=c('rsubset','rfMod','rftest')}
importantVariables2
```

The random forest method entails cross-validation in order to select a final model; no additional cross-validation is required. As indicated below, the final model entailed cross-validation accuracy of 0.975; its OOB error rate for the selected model was 0.025. That is my expectation for the error rate on any test sets.

To verify this expectation, I tested the final model on training subsets 2, 3, 4, and 5. Their error rates ranged from to .

```{r sumRF2,cache=TRUE,echo=TRUE,dependson=c('rsubset','rfMod','rftest')}

confusionMatrix(predict(rfImportant, subSet2), subSet2$classe)
confusionMatrix(predict(rfImportant, subSet3), subSet3$classe)
confusionMatrix(predict(rfImportant, subSet4), subSet4$classe)
confusionMatrix(predict(rfImportant, subSet5), subSet5$classe)

```

I would predict accuracy of 0.98 on the testing data. The chance of predicting all 20 correctly is 0.98^20, or 0.67.