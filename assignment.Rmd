---
title: "Predicting Exercise Class from Activity Measurements"
author: "TMA"
date: "Tuesday, August 19, 2014"
output:
  html_document:
    highlight: kate
    theme: cerulean
---
========================================

```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
library(knitr)
options(width = 100)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, cache=TRUE, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```


## 1. Summary
The goal of this task was to predict the manner of exercise performed by participants in the Weight Lifting Exercises (WLE) Dataset <http://groupware.les.inf.puc-rio.br/har>.

I compared two approaches for modeling predictions, both using the random forest method. In the first approach I trained a single random forest model on the entire training set. 

The second approach took two steps. First I identified the most important variables within four distinct random forest models. Then I built a random forest model on the entire data set using only the variables identified as most important in the four models in the first step.

Both approaches resulted in accuracy that rounds to 1.0. The expected out of sample error rates were 0.41% and 0.35%, respectively.


## 2. Data Processing
### The experiment
The WLE data describe an experiment in which six human participants performed exercises known as _dumbbell biceps curls_. The experimenters instructed the participants to perform the exercises either properly or improperly, as indicated by five different classes: Class A indicated proper form, while Classes B, C, D, and E indicated improper form, instructed in specific ways.

The experimenters measured participants' activity from four sensors in different locations, including on three wearable devices. The locations were on the dumbbells themselves, on a belt worn around the waist, on an arm-band, and on a glove. Each device's metrics included tri-axis measures from an accelerometer, a gyroscope, and a magnetometer.

To read more, visit the WLE website: <http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf>.

### The data
The data consists of a preprocessed training data set including 19,622 observations among 160 variables, and a testing data set including 20 observations. The testing data did not label the "classe" variable, the discovery of which was the purpose of this analysis. 

#### Removing non-predictive variables
To eliminate variables not useful in the testing data, I subset the training data to include only variables that did not have "NA" in the testing data, leaving 52 possible predictor variables. These possible predictor variables comprised 13 variables for each of the four measurement locations.

```{r rsubset,cache=TRUE,echo=FALSE}
library(knitr); library(caret); library(randomForest); library(gbm);
library(party);library(ElemStatLearn); library(xtable);
library(MASS);library(plyr); library(survival); library(splines);

setwd("~/GitHub/pml")
training <- read.csv("~/GitHub/pml/pml-training.csv")
testing <- read.csv("~/GitHub/pml/pml-testing.csv")

allColumns <- colnames(training)

# id columns that are not NA in testing data set
availColumns <- cbind(colnames(training),apply(testing, 2, max))
availColumns <- availColumns[is.na(availColumns[,2])==FALSE ,1]
availTrain <- training[, cbind(availColumns)[-c(1,3:6)]]
availTest <- testing[, colnames(testing) %in% colnames(availTrain)]

trainRows <- nrow(availTrain)

wleVariables <- matrix(rep("", 52), nrow=13, ncol = 4)
wleVariables[,1] <- colnames(availTrain)[3:15]
wleVariables[,2] <- colnames(availTrain)[16:28]
wleVariables[,3] <- colnames(availTrain)[29:41]
wleVariables[,4] <- colnames(availTrain)[42:54]
colnames(wleVariables) <- c("Belt", "Arm", "Dumbbells", "Forearm")

set.seed(80302)
subRows1 <- sample(1:trainRows, trainRows/5)
subSet1 <- availTrain[subRows1,]

subRows2 <- sample((1:trainRows)[!(row(availTrain)[,1] %in% subRows1)], (trainRows - length(subRows1))/4)
subSet2 <- availTrain[subRows2,]

subRows3 <- sample((1:trainRows)[!(row(availTrain)[,1] %in% c(subRows1, subRows2))], (trainRows - length(subRows1) - length(subRows2))/3)
subSet3 <- availTrain[subRows3,]

subRows4 <- sample((1:trainRows)[!(row(availTrain)[,1] %in% c(subRows1, subRows2,subRows3))], (trainRows - length(subRows1) - length(subRows2) - length(subRows3))/2)
subSet4 <- availTrain[subRows4,]

subSet5 <- availTrain[(1:trainRows)[!(row(availTrain)[,1] %in% c(subRows1, subRows2,subRows3,subRows4))],]

```

##### Table 1: Predictive Variables
```{r rfSub2,cache=FALSE,echo=FALSE,dependson='rsubset',results='asis'}
library(xtable)
xtab1 <- xtable(wleVariables)
print(xtab1, type = "html", include.rownames = FALSE)
```

I also subset 20% of the training data by randomly sampling the "classe" (outcome) variable. I employed this subset to train four location-specific random forest models, as described in Section 3.

```{r rfMod,cache=TRUE,echo=FALSE,dependson='rsubset'}
set.seed(60614)
rfBelt <- train(classe ~ ., method="rf",data=subSet1[,c(3:15,55)],verbose=FALSE)
rfArm <- train(classe ~ ., method="rf",data=subSet1[,c(16:28,55)],verbose=FALSE)
rfDumbell <- train(classe ~ ., method="rf",data=subSet1[,c(29:41,55)],verbose=FALSE)
rfForearm <- train(classe ~ ., method="rf",data=subSet1[,c(42:54,55)],verbose=FALSE)
rfTrain  <- train(classe ~ ., method="rf",data=availTrain[,c(3:54,55)],verbose=FALSE)
```

## 3. Modeling
### Model 1: _rfTrain_
I trained a random forest model on the entire training set using the default parameters from the caret package.

### Model 2: _rfTrain2_
I also trained four random forest models on a training data subset which comprised 20% of the training data, using the default parameters from the caret package. I confined each model to a specific device location.

Combining the most important variables from each location - defining "important" as greater than 50 - I trained a random forest model on the entire training data set using only the important variables and using the default parameters from the caret package.

## 4. Results
### Important variables
The random forest method grows trees randomly on subsets of the training data and selects the best one based on out of the bag (OOB) cross-validation error rates, so there is no need for the modeler to hold out data to conduct an additional cross validation exercise outside the random forest process.

Among the four mini random forest models, the following variables met the threshold of variable importance greater than 50.

##### Table 2. Most important variables from discrete location models
```{r rfInspect,cache=TRUE,echo=FALSE,dependson=c('rsubset','rfMod'),results='asis'}
library(xtable)

varImpOverall <- rbind(varImp(rfBelt)$importance, varImp(rfArm)$importance, varImp(rfDumbell)$importance, varImp(rfForearm)$importance)

importantVariables2a <- rownames(varImpOverall)[varImpOverall$Overall > 50]

importantVariables2 <- c(rownames(varImpOverall)[varImpOverall$Overall > 50],"classe")

impVar2Matrix <- matrix(importantVariables2a, ncol = 1)
colnames(impVar2Matrix) <- "All locations"
xtable2 <- xtable(impVar2Matrix)

print(xtable2, type = 'html', include.rownames = FALSE)

```

```{r rfTest2,cache=TRUE,echo=FALSE,dependson=c('rsubset','rfMod')}
rfTrain2  <- train(classe ~ ., method="rf",data=availTrain[,importantVariables2],verbose=FALSE)

```

### rfTrain results
_rfTrain_ is the model generated in a single step, training a random forest model on the entire training data set among all 52 predictor variables. Its predicted error rate, estimated from the OOB error rate used to select the best tree, was 0.41%.

```{r plot1,cache=TRUE,echo=FALSE,dependson=c('rsubset','rfMod', 'rfTest2'),fig.align='center',fig.width=10,fig.height=8}
library(caret)
rfTrain$finalModel

```

### rfTrain2 results
_rfTrain2_ is the model generated in two steps, with the first step training four random forest models on 20% of the training data, with each model confined to 13 predictor variables from one device location. The second step consisted of training a single random forest model on the entire training set using only variables whose importance was greater than 50 in the first step. Its predicted error rate, estimated from the OOB error rate used to select the best tree, was 0.35%.

```{r plot2,cache=TRUE,echo=FALSE,dependson=c('rsubset','rfMod', 'rfTest2'),fig.align='center',fig.width=10,fig.height=8}
library(caret)
rfTrain2$finalModel

```

## 5. Error prediction
The OOB error rate is an unbiased estimate of out of sample error rate (see <http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#ooberr>), so there is no need for additional cross validation.

For either model, the expected likelihood of predicting all 20 test cases correctly on using _rfTrain2_ is approximately 0.9965^20, or 0.93. 

========================================

## Appendix

### Figure 1. Most Important Variables, _rfTrain_
```{r plot3,cache=TRUE,echo=FALSE,dependson=c('rsubset','rfMod', 'rfTest2'),fig.align='center',fig.width=10,fig.height=8}
library(caret)
varImp1 <- varImp(rfTrain)
plot(varImp1, top = 10)

```

### Figure 2. Most Important Variables, _rfTrain2_
```{r plot4,cache=TRUE,echo=FALSE,dependson=c('rsubset','rfMod', 'rfTest2'),fig.align='center',fig.width=10,fig.height=8}
library(caret)
varImp2 <- varImp(rfTrain)
plot(varImp2, top = 10)

```
